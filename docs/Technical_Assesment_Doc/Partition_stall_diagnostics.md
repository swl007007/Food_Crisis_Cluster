Why Partitions Stall
                                                                                                                                                                                                                                                                                                                                                                                                    
  - Validation coverage is random. GeoRF.fit calls train_val_split, which tags individual rows without caring about spatial grouping (src/model/GeoRF.py:196). Any admin code that never lands in the val fold simply keeps the root model forever because get_branch_data_by_group can only reassign groups that appear in val_list (src/helper/helper.py:198). Ensuring each FEWSNET_admin_code   
  contributes validation samples (e.g., pass your own X_set that withholds the latest quarter per unit) usually unlocks partitions.                                                                                                                                                                                                                                                                 
  - Positive-sample gating is tight. The scan stage bails when the aggregated validation signal is tiny: MIN_SCAN_CLASS_SAMPLE and MIN_BRANCH_SAMPLE_SIZE require at least a handful of class‑1 hits in the candidate branch (src/partition/transformation.py:259, src/helper/helper.py:37). With rare crises, many candidate splits never even reach the significance test. Dropping those         
  thresholds—or feeding more val positives via stratified sampling—gives the optimizer something to work with.                                                                                                                                                                                                                                                                                      
  - The metric proxy is ultra-sparse. With CRISIS_FOCUSED_OPTIMIZATION=True, get_score and sig_test count only class‑1 true positives (they literally return 1 for a TP, 0 otherwise in src/partition/partition_opt.py:28 and src/tests/sig_test.py:17). If a branch’s validation slice has three crisis records, the best possible “improvement” is +3. That makes the effect-size and mean-diff   
  tests almost always fail even though overall accuracy/recall may shift. Consider widening the validation window (e.g., multiple quarters) or temporarily disabling the crisis-only pathway to see whether partitions appear under the standard accuracy metric.                                                                                                                                   
  - Contiguity cleanup can erase splits. After scan finds s0/s1, get_refined_partitions_dispatcher and the polygon smoothing may vote marginalized polygons back into the majority partition (src/partition/transformation.py:283). In strongly imbalanced or widely spaced data the refinement loop can undo most of the split signal before you even hit significance. Try a run with             
  CONTIGUITY=False (or REFINE_TIMES = 0) to confirm the raw split actually helped.                                                                                                                                                                                                                                                                                                                  
  - Finally, remember that many groups never retrain. The branch table is populated with whatever groups were present in the validation set at split time (src/partition/transformation.py:655). Any admin that wasn’t in the val fold inherits the parent branch (get_X_branch_id_by_group, src/helper/helper.py:284), so your test set may still be routed to the root forest, producing identical
  metrics. Again, providing a deterministic X_set that gives every admin a validation sample fixes this.                                                                                                                                                                                                                                                                                            

                                                                                                                                                                                                                                                                                                                                                       Besides explicitly forcing each group has at least one validation sample, we probably can use bootstrap to explicitly calculate a more robust per-group error metric and use this error metric(along with its s.d.) as input of scan algorithm. After a LTSS/q value is generated, we can again bootstrap the child/parent  branch validation to calculate RF/significant test logic.  To try it, we’d need to:
- replace the aggregation step so get_class_wise_stat (or a sibling) returns mean/s.d. of a bootstrap distribution per X_group rather than a raw count. The scan objective in partition_opt.scan would then rank groups by, say, average accuracy score or F1 score instead of the current difference vector.
- feed those bootstrapped summaries into sig_test: rather than comparing two 0/1 arrays, you compute parent/child distributions via resampling, derive the LTSS/q value you mentioned, and run a bootstrap t-test or percentile interval to decide the split. Same wrapper could be reused for the final temporal evaluation to report confidence intervals.
Implementation-wise that means new helper functions for per-branch bootstrapping, adjustments in partition(...) to persist the bootstrap stats, and a rewrite of the confidence test to support the richer inputs. If you want to pursue it we can sketch the data structures and update paths to slot the bootstrap summaries into the existing loop. Let me know where you’d like to start. 

Natural next steps: 1) craft a grouped/temporal X_set so each admin has validation coverage; 2) rerun with CONTIGUITY=False and smaller MIN_SCAN_CLASS_SAMPLE to see if splits start moving the class‑1 F1; 3) log mean_diff, test_stat, and partition_type from sig_test/metrics_tracker to spot which gating step stops each branch. 