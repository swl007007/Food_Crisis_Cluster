{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aec09ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @Author: xie\n",
    "# @Date:   2025-06-20\n",
    "# @Email:  xie@umd.edu\n",
    "# @Last modified by:   xie\n",
    "# @Last modified time: 2025-06-20\n",
    "# @License: MIT License\n",
    "\n",
    "'''\n",
    "GeoRF_main.py\n",
    "\n",
    "This file shows an example on how to define, train and evaluate a GeoRF model.\n",
    "The general usage is similar to standard machine learning models, with fit(), predict() and evaluate() functions.\n",
    "The main difference is that GeoRF is a spatially-explicit model, which means location info is needed as part of inputs.\n",
    "\n",
    "Usage:\n",
    "\t\tThere are 4 key steps:\n",
    "\t\t\t(1) Data loading: Prepare standard X, y, as well as locations X_loc\n",
    "\t\t\t(2) Define groups using GroupGenerator() or by customizing it for your own data: This is needed in GeoRF, which defines the minimum spatial unit for space partitioning.\n",
    "\t\t\t\t\tFor example, a grid can be used to split the study area into groups, where all data points in each grid cell form a group.\n",
    "\t\t\t\t\tThe data may contain thousands of groups, which may be partitioned by GeoRF into 10-20 partitions (one RF per partition). This is just to give a sense of the quantities.\n",
    "\t\t\t\t\tIn testing, the grouping is used to assign test data points to local models (one local RF model is learned for each partition).\n",
    "\t\t\t(3) Training: georf.fit()\n",
    "\t\t\t(4) Prediction and evaluation: georf.predict() and georf.evaluate()\n",
    "\n",
    "\t\tFor visualizations, code for grid-based grouping is provided. For other group definitions, please write customized visualization functions.\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "import sys\n",
    "#GeoRF\n",
    "#Can be customized with the template\n",
    "from GeoRF import GeoRF\n",
    "from customize import *\n",
    "from data import load_demo_data#load_data_us_cdl\n",
    "from helper import get_spatial_range\n",
    "from initialization import train_test_split_all\n",
    "#All global parameters\n",
    "from config import *\n",
    "from sklearn.impute import SimpleImputer\n",
    "import polars as pl\n",
    "# import machine learning metrics, precision, recall and f1_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "data = pl.read_csv(r\"C:\\Users\\swl00\\IFPRI Dropbox\\Weilun Shi\\Google fund\\Analysis\\1.Source Data\\FEWSNET_IPC_train_lag_forecast_v06252025.csv\")\n",
    "# drop ISO3, fews_ipc_adjusted, fews_proj_med_adjusted, fews_ipc,fews_proj_near, fews_proj_near_ha, fews_proj_med, fews_proj_med_ha, ADMIN0, ADMIN1,ADMIN2,ADMIN3,ISO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "723c30ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# customize function\n",
    "def comp_impute(X, strategy = \"max_plus\", multiplier = 100.0):\n",
    "    # loop over X columns, replace inf with missing\n",
    "    for _ in range(X.shape[1]):\n",
    "        print(f\"Checking column {_} for inf values\")\n",
    "        # Try to convert column to float, ignore errors (non-convertible values remain unchanged)\n",
    "        col_data = X[:, _]\n",
    "        try:\n",
    "            col_data_float = col_data.astype(float)\n",
    "            if np.isinf(col_data_float).any():\n",
    "                print(f\"Column {_} has inf values, replacing with NaN\")\n",
    "                col_data_float[np.isinf(col_data_float)] = np.nan\n",
    "                X[:, _] = col_data_float\n",
    "        except Exception as e:\n",
    "            print(f\"Column {_} could not be converted to float: {e}\")\n",
    "            # Skip columns that cannot be converted\n",
    "            continue\n",
    "    X_imputed, imputer = impute_missing_values(X, strategy=strategy, multiplier=multiplier, verbose=True)\n",
    "    X = X_imputed\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff2dfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine xmin,xmax,ymin and ymax first before runing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3a880ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "cols_to_drop = [\n",
    "        \"ISO3\", \"fews_ipc_adjusted\", \"fews_proj_med_adjusted\", \"fews_ipc\",\n",
    "        \"fews_proj_near\", \"fews_proj_near_ha\", \"fews_proj_med\",\n",
    "        \"fews_proj_med_ha\", \"ADMIN0\", \"ADMIN1\", \"ADMIN2\", \"ADMIN3\"\n",
    "    ]\n",
    "data = data.drop(cols_to_drop)\n",
    "less_var = True\n",
    "if less_var:\n",
    "    var_extra = ['Evap_tavg_mean',\n",
    "    'Qsb_tavg_mean',\n",
    "    'RadT_tavg_mean',\n",
    "    'SnowCover_inst_mean',\n",
    "    'SnowDepth_inst_mean',\n",
    "    'Snowf_tavg_mean',\n",
    "    'SoilMoi00_10cm_tavg_mean',\n",
    "    'SoilMoi10_40cm_tavg_mean',\n",
    "    'SoilMoi100_200cm_tavg_mean',\n",
    "    'SoilMoi40_100cm_tavg_mean',\n",
    "    'LWdown_f_tavg_mean',\n",
    "    'SoilTemp00_10cm_tavg_mean',\n",
    "    'SoilTemp10_40cm_tavg_mean',\n",
    "    'SoilTemp100_200cm_tavg_mean',\n",
    "    'SoilTemp40_100cm_tavg_mean',\n",
    "    'SWdown_f_tavg_mean',\n",
    "    'SWE_inst_mean',\n",
    "    'Swnet_tavg_mean',\n",
    "    'Wind_f_tavg_mean',\n",
    "    'Lwnet_tavg_mean',\n",
    "    'Psurf_f_tavg_mean',\n",
    "    'Qair_f_tavg_mean',\n",
    "    'Qg_tavg_mean',\n",
    "    'Qh_tavg_mean',\n",
    "    'Qle_tavg_mean',\n",
    "    'Qs_tavg_mean']\n",
    "    # drop any columns start with var_extra\n",
    "    data = data.drop([col for col in data.columns if col.startswith(tuple(var_extra))])\n",
    "    \n",
    "\n",
    "\n",
    "# keep only fews_ipc_crisis is not null\n",
    "data = data.filter(pl.col(\"fews_ipc_crisis\").is_not_null())\n",
    "\n",
    "# encode ISO\n",
    "data = data.with_columns([\n",
    "    pl.col(\"ISO\").cast(pl.Categorical).to_physical().alias(\"ISO_encoded\")\n",
    "])\n",
    "\n",
    "# drop unit name, ISO\n",
    "data = data.drop([\"unit_name\", \"ISO\"])\n",
    "# for AEZ_* replace \"False\" with 0 and \"True\" with 1\n",
    "for col in data.columns:\n",
    "    if col.startswith(\"AEZ_\"):\n",
    "        data = data.with_columns(\n",
    "            pl.when(pl.col(col) == \"True\")\n",
    "            .then(1)\n",
    "            .when(pl.col(col) == \"False\")\n",
    "            .then(0)\n",
    "            .otherwise(pl.col(col))\n",
    "            .alias(col)\n",
    "        )\n",
    "\n",
    "\n",
    "df = data.to_pandas()\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['years'] = df['date'].dt.year\n",
    "df['fews_ipc_crisis_lag_1'] = df.groupby('FEWSNET_admin_code')['fews_ipc_crisis'].shift(1)\n",
    "df['fews_ipc_crisis_lag_2']= df.groupby('FEWSNET_admin_code')['fews_ipc_crisis'].shift(2)\n",
    "df['fews_ipc_crisis_lag_3'] = df.groupby('FEWSNET_admin_code')['fews_ipc_crisis'].shift(3)\n",
    "# generate dummys for year month\n",
    "for year in df['years'].unique():\n",
    "    df[f'year_{year}'] = (df['years'] == year).astype(int)\n",
    "for month in df['date'].dt.month.unique():\n",
    "    df[f'month_{month}'] = (df['date'].dt.month == month).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_loc = df[['lat', 'lon']].values\n",
    "\n",
    "\n",
    "\n",
    "aez_columns = [col for col in df.columns if col.startswith('AEZ_')]\n",
    "df['AEZ_group'] = df.groupby(aez_columns).ngroup()\n",
    "df['AEZ_country_group'] = df.groupby(['AEZ_group', 'ISO_encoded']).ngroup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a5f7cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using features for clustering: ['WFP_Price_std']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swl00\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (100). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 100 clusters from 4081 unique admin codes\n",
      "Cluster sizes range from 1 to 1532 admin codes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#prepare groups (minimum spatial units for space-partitioning and location groupings, e.g., using a grid)\n",
    "assignment = 'all_kmeans' # if polygons is True, use FEWSNET_admin_code as groups, otherwise use grid-based grouping\n",
    "#extract the values of FEWSNET_admin_code and assign to X_group\n",
    "if assignment=='polygons':\n",
    "    X_group = df['FEWSNET_admin_code'].values\n",
    "    X_group = X_group.astype(int)\n",
    "elif assignment=='grid':\n",
    "    #prepare groups (minimum spatial units for space-partitioning and location groupings, e.g., using a grid)\n",
    "    xmin, xmax, ymin, ymax = get_spatial_range(X_loc)\n",
    "    STEP_SIZE = 0.1\n",
    "    group_gen = GroupGenerator(xmin, xmax, ymin, ymax, STEP_SIZE)\n",
    "    X_group = group_gen.get_groups(X_loc)\n",
    "elif assignment=='country':\n",
    "    X_group = df['ISO_encoded'].values\n",
    "    X_group = X_group.astype(int)\n",
    "elif assignment=='AEZ':\n",
    "    X_group = df['AEZ_group'].values\n",
    "    X_group = X_group.astype(int)\n",
    "elif assignment=='country_AEZ':\n",
    "    X_group = df['AEZ_country_group'].values\n",
    "    X_group = X_group.astype(int)\n",
    "elif assignment=='geokmeans':\n",
    "    cluster_id, cluster_info, admin_to_group_map = create_kmeans_groupgenerator_from_admin_codes(df, n_clusters=100, random_state=42, features_for_clustering=['lat', 'lon'])\n",
    "    X_group=cluster_id\n",
    "    X_group = X_group.astype(int)\n",
    "    # from admin_to_group_map, create a correspondence table, its index is FEWSNET_admin_code, and column is cluster_id\n",
    "    correspondence_table = pd.DataFrame(list(admin_to_group_map.items()), columns=['FEWSNET_admin_code', 'cluster_id'])\n",
    "    correspondence_table.to_csv('correspondence_table_geokmeans.csv', index=False)\n",
    "elif assignment=='all_kmeans':\n",
    "    X_columns = df.drop(columns=['FEWSNET_admin_code', 'lat', 'lon', 'fews_ipc_crisis']).columns.tolist()\n",
    "    cluster_id, cluster_info, admin_to_group_map = create_kmeans_groupgenerator_from_admin_codes(df, n_clusters=100, random_state=42, features_for_clustering=X_columns)\n",
    "    X_group = cluster_id\n",
    "    X_group = X_group.astype(int)\n",
    "    correspondence_table = pd.DataFrame(list(admin_to_group_map.items()), columns=['FEWSNET_admin_code', 'cluster_id'])\n",
    "    correspondence_table.to_csv('correspondence_table_allkmeans.csv', index=False)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown assignment method: {assignment}\")\n",
    "y = df['fews_ipc_crisis'].values\n",
    "\n",
    "\n",
    "# save correspondence table\n",
    "correspondence_table = df[['FEWSNET_admin_code', 'AEZ_group', 'AEZ_country_group', 'ISO_encoded']].drop_duplicates()\n",
    "correspondence_table.to_csv('correspondence_table.csv', index=False)\n",
    "#X impute with column mean\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# sort by FEWSNET_admin_code and date\n",
    "df = df.sort_values(by=['FEWSNET_admin_code', 'date'])\n",
    "# for each FEWSNET_admin_code, created lagged fews_ipc_crisis for 1,2 and 3 terms\n",
    "for lag in range(1, 4):\n",
    "    df[f'fews_ipc_crisis_lag_{lag}'] = df.groupby('FEWSNET_admin_code')['fews_ipc_crisis'].shift(lag)\n",
    "# drop date\n",
    "df = df.drop(columns=['date'])\n",
    "df=df.drop(columns=['fews_ipc_crisis','AEZ_group','ISO_encoded','AEZ_country_group'])\n",
    "time_variants =  ['event_count_battles', 'event_count_explosions', 'event_count_violence', 'sum_fatalities_battles', 'sum_fatalities_explosions', 'sum_fatalities_violence', 'event_count_battles_w5', 'event_count_explosions_w5', 'event_count_violence_w5', 'sum_fatalities_battles_w5', 'sum_fatalities_explosions_w5', 'sum_fatalities_violence_w5', 'event_count_battles_w10', 'event_count_explosions_w10', 'event_count_violence_w10', 'sum_fatalities_battles_w10', 'sum_fatalities_explosions_w10', 'sum_fatalities_violence_w10', 'nightlight', 'nightlight_sd', 'EVI', 'EVI_stdDev',  'FAO_price', 'Evap_tavg_mean', 'Evap_tavg_stdDev', 'LWdown_f_tavg_mean', 'LWdown_f_tavg_stdDev', 'Lwnet_tavg_mean', 'Lwnet_tavg_stdDev', 'Psurf_f_tavg_mean', 'Psurf_f_tavg_stdDev', 'Qair_f_tavg_mean', 'Qair_f_tavg_stdDev', 'Qg_tavg_mean', 'Qg_tavg_stdDev', 'Qh_tavg_mean', 'Qh_tavg_stdDev', 'Qle_tavg_mean', 'Qle_tavg_stdDev', 'Qs_tavg_mean', 'Qs_tavg_stdDev', 'Qsb_tavg_mean', 'Qsb_tavg_stdDev', 'RadT_tavg_mean', 'RadT_tavg_stdDev', 'Rainf_f_tavg_mean', 'Rainf_f_tavg_stdDev', 'SnowCover_inst_mean', 'SnowCover_inst_stdDev', 'SnowDepth_inst_mean', 'SnowDepth_inst_stdDev', 'Snowf_tavg_mean', 'Snowf_tavg_stdDev', 'SoilMoi00_10cm_tavg_mean', 'SoilMoi00_10cm_tavg_stdDev', 'SoilMoi10_40cm_tavg_mean', 'SoilMoi10_40cm_tavg_stdDev', 'SoilMoi100_200cm_tavg_mean', 'SoilMoi100_200cm_tavg_stdDev', 'SoilMoi40_100cm_tavg_mean', 'SoilMoi40_100cm_tavg_stdDev', 'SoilTemp00_10cm_tavg_mean', 'SoilTemp00_10cm_tavg_stdDev', 'SoilTemp10_40cm_tavg_mean', 'SoilTemp10_40cm_tavg_stdDev', 'SoilTemp100_200cm_tavg_mean', 'SoilTemp100_200cm_tavg_stdDev', 'SoilTemp40_100cm_tavg_mean', 'SoilTemp40_100cm_tavg_stdDev', 'SWdown_f_tavg_mean', 'SWdown_f_tavg_stdDev', 'SWE_inst_mean', 'SWE_inst_stdDev', 'Swnet_tavg_mean', 'Swnet_tavg_stdDev', 'Tair_f_tavg_mean', 'Tair_f_tavg_stdDev', 'Wind_f_tavg_mean', 'Wind_f_tavg_stdDev', 'gpp_sd', 'gpp_mean', 'CPI', 'GDP', 'CC', 'gini', 'WFP_Price', 'WFP_Price_std']\n",
    "time_variants_m12 = [variant + '_m12' for variant in time_variants]\n",
    "time_variants_list = time_variants + time_variants_m12\n",
    "l2_index = []\n",
    "# if columns in time_variants_list, record its index and restore in l2_index\n",
    "for i, col in enumerate(df.columns):\n",
    "    if col in time_variants_list:\n",
    "        l2_index.append(i)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dcb3d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking column 0 for inf values\n",
      "Checking column 1 for inf values\n",
      "Checking column 2 for inf values\n",
      "Checking column 3 for inf values\n",
      "Checking column 4 for inf values\n",
      "Checking column 5 for inf values\n",
      "Checking column 6 for inf values\n",
      "Checking column 7 for inf values\n",
      "Checking column 8 for inf values\n",
      "Checking column 9 for inf values\n",
      "Checking column 10 for inf values\n",
      "Checking column 11 for inf values\n",
      "Checking column 12 for inf values\n",
      "Checking column 13 for inf values\n",
      "Checking column 14 for inf values\n",
      "Checking column 15 for inf values\n",
      "Checking column 16 for inf values\n",
      "Checking column 17 for inf values\n",
      "Checking column 18 for inf values\n",
      "Checking column 19 for inf values\n",
      "Checking column 20 for inf values\n",
      "Checking column 21 for inf values\n",
      "Checking column 22 for inf values\n",
      "Checking column 23 for inf values\n",
      "Checking column 24 for inf values\n",
      "Checking column 25 for inf values\n",
      "Checking column 26 for inf values\n",
      "Checking column 27 for inf values\n",
      "Checking column 28 for inf values\n",
      "Checking column 29 for inf values\n",
      "Checking column 30 for inf values\n",
      "Checking column 31 for inf values\n",
      "Checking column 32 for inf values\n",
      "Checking column 33 for inf values\n",
      "Checking column 34 for inf values\n",
      "Checking column 35 for inf values\n",
      "Checking column 36 for inf values\n",
      "Checking column 37 for inf values\n",
      "Checking column 38 for inf values\n",
      "Checking column 39 for inf values\n",
      "Checking column 40 for inf values\n",
      "Checking column 41 for inf values\n",
      "Checking column 42 for inf values\n",
      "Checking column 43 for inf values\n",
      "Checking column 44 for inf values\n",
      "Checking column 45 for inf values\n",
      "Checking column 46 for inf values\n",
      "Checking column 47 for inf values\n",
      "Checking column 48 for inf values\n",
      "Checking column 49 for inf values\n",
      "Checking column 50 for inf values\n",
      "Checking column 51 for inf values\n",
      "Checking column 52 for inf values\n",
      "Checking column 53 for inf values\n",
      "Checking column 54 for inf values\n",
      "Checking column 55 for inf values\n",
      "Checking column 56 for inf values\n",
      "Checking column 57 for inf values\n",
      "Checking column 58 for inf values\n",
      "Checking column 59 for inf values\n",
      "Checking column 60 for inf values\n",
      "Checking column 61 for inf values\n",
      "Checking column 62 for inf values\n",
      "Checking column 63 for inf values\n",
      "Checking column 64 for inf values\n",
      "Checking column 65 for inf values\n",
      "Checking column 66 for inf values\n",
      "Checking column 67 for inf values\n",
      "Checking column 68 for inf values\n",
      "Checking column 69 for inf values\n",
      "Checking column 70 for inf values\n",
      "Checking column 71 for inf values\n",
      "Checking column 72 for inf values\n",
      "Checking column 73 for inf values\n",
      "Checking column 74 for inf values\n",
      "Checking column 75 for inf values\n",
      "Checking column 76 for inf values\n",
      "Checking column 77 for inf values\n",
      "Checking column 78 for inf values\n",
      "Checking column 79 for inf values\n",
      "Checking column 80 for inf values\n",
      "Checking column 81 for inf values\n",
      "Checking column 82 for inf values\n",
      "Checking column 83 for inf values\n",
      "Checking column 84 for inf values\n",
      "Checking column 85 for inf values\n",
      "Checking column 86 for inf values\n",
      "Checking column 87 for inf values\n",
      "Checking column 88 for inf values\n",
      "Checking column 89 for inf values\n",
      "Checking column 90 for inf values\n",
      "Checking column 91 for inf values\n",
      "Checking column 92 for inf values\n",
      "Checking column 93 for inf values\n",
      "Checking column 94 for inf values\n",
      "Checking column 95 for inf values\n",
      "Checking column 96 for inf values\n",
      "Checking column 97 for inf values\n",
      "Checking column 98 for inf values\n",
      "Checking column 99 for inf values\n",
      "Checking column 100 for inf values\n",
      "Checking column 101 for inf values\n",
      "Checking column 102 for inf values\n",
      "Checking column 103 for inf values\n",
      "Checking column 104 for inf values\n",
      "Checking column 105 for inf values\n",
      "Checking column 106 for inf values\n",
      "Checking column 107 for inf values\n",
      "Checking column 108 for inf values\n",
      "Checking column 109 for inf values\n",
      "Checking column 110 for inf values\n",
      "Checking column 111 for inf values\n",
      "Checking column 112 for inf values\n",
      "Checking column 113 for inf values\n",
      "Checking column 114 for inf values\n",
      "Checking column 115 for inf values\n",
      "Checking column 116 for inf values\n",
      "Checking column 117 for inf values\n",
      "Checking column 118 for inf values\n",
      "Checking column 119 for inf values\n",
      "Checking column 120 for inf values\n",
      "Checking column 121 for inf values\n",
      "Checking column 122 for inf values\n",
      "Checking column 123 for inf values\n",
      "Checking column 124 for inf values\n",
      "Checking column 125 for inf values\n",
      "Checking column 126 for inf values\n",
      "Checking column 127 for inf values\n",
      "Checking column 128 for inf values\n",
      "Checking column 129 for inf values\n",
      "Checking column 130 for inf values\n",
      "Checking column 131 for inf values\n",
      "Checking column 132 for inf values\n",
      "Checking column 133 for inf values\n",
      "Checking column 134 for inf values\n",
      "Checking column 135 for inf values\n",
      "Checking column 136 for inf values\n",
      "Checking column 137 for inf values\n",
      "Checking column 138 for inf values\n",
      "Checking column 139 for inf values\n",
      "Checking column 140 for inf values\n",
      "Checking column 141 for inf values\n",
      "Checking column 142 for inf values\n",
      "Checking column 143 for inf values\n",
      "Checking column 144 for inf values\n",
      "Checking column 145 for inf values\n",
      "Checking column 146 for inf values\n",
      "Checking column 147 for inf values\n",
      "Checking column 148 for inf values\n",
      "Checking column 149 for inf values\n",
      "Checking column 150 for inf values\n",
      "Checking column 151 for inf values\n",
      "Checking column 152 for inf values\n",
      "Checking column 153 for inf values\n",
      "Checking column 154 for inf values\n",
      "Checking column 155 for inf values\n",
      "Checking column 156 for inf values\n",
      "Checking column 157 for inf values\n",
      "Checking column 158 for inf values\n",
      "Checking column 159 for inf values\n",
      "Checking column 160 for inf values\n",
      "Checking column 161 for inf values\n",
      "Checking column 162 for inf values\n",
      "Checking column 163 for inf values\n",
      "Column 163 has inf values, replacing with NaN\n",
      "Checking column 164 for inf values\n",
      "Column 164 has inf values, replacing with NaN\n",
      "Checking column 165 for inf values\n",
      "Checking column 166 for inf values\n",
      "Checking column 167 for inf values\n",
      "Checking column 168 for inf values\n",
      "Checking column 169 for inf values\n",
      "Checking column 170 for inf values\n",
      "Checking column 171 for inf values\n",
      "Checking column 172 for inf values\n",
      "Checking column 173 for inf values\n",
      "Checking column 174 for inf values\n",
      "Checking column 175 for inf values\n",
      "Checking column 176 for inf values\n",
      "Checking column 177 for inf values\n",
      "Checking column 178 for inf values\n",
      "Checking column 179 for inf values\n",
      "Checking column 180 for inf values\n",
      "Checking column 181 for inf values\n",
      "Checking column 182 for inf values\n",
      "Checking column 183 for inf values\n",
      "Checking column 184 for inf values\n",
      "Checking column 185 for inf values\n",
      "Checking column 186 for inf values\n",
      "Checking column 187 for inf values\n",
      "Checking column 188 for inf values\n",
      "Checking column 189 for inf values\n",
      "Checking column 190 for inf values\n",
      "Checking column 191 for inf values\n",
      "Checking column 192 for inf values\n",
      "Checking column 193 for inf values\n",
      "Checking column 194 for inf values\n",
      "Checking column 195 for inf values\n",
      "Checking column 196 for inf values\n",
      "Checking column 197 for inf values\n",
      "Checking column 198 for inf values\n",
      "Checking column 199 for inf values\n",
      "Checking column 200 for inf values\n",
      "Checking column 201 for inf values\n",
      "Checking column 202 for inf values\n",
      "Checking column 203 for inf values\n",
      "Checking column 204 for inf values\n",
      "Checking column 205 for inf values\n",
      "Checking column 206 for inf values\n",
      "Checking column 207 for inf values\n",
      "Checking column 208 for inf values\n",
      "Checking column 209 for inf values\n",
      "Checking column 210 for inf values\n",
      "Checking column 211 for inf values\n",
      "Checking column 212 for inf values\n",
      "Checking column 213 for inf values\n",
      "Checking column 214 for inf values\n",
      "Checking column 215 for inf values\n",
      "Checking column 216 for inf values\n",
      "Checking column 217 for inf values\n",
      "Checking column 218 for inf values\n",
      "Checking column 219 for inf values\n",
      "Checking column 220 for inf values\n",
      "Checking column 221 for inf values\n",
      "Checking column 222 for inf values\n",
      "Checking column 223 for inf values\n",
      "Checking column 224 for inf values\n",
      "Checking column 225 for inf values\n",
      "Checking column 226 for inf values\n",
      "Checking column 227 for inf values\n",
      "Checking column 228 for inf values\n",
      "Checking column 229 for inf values\n",
      "Checking column 230 for inf values\n",
      "Checking column 231 for inf values\n",
      "Checking column 232 for inf values\n",
      "Checking column 233 for inf values\n",
      "Checking column 234 for inf values\n",
      "Checking column 235 for inf values\n",
      "Checking column 236 for inf values\n",
      "Checking column 237 for inf values\n",
      "Checking column 238 for inf values\n",
      "Checking column 239 for inf values\n",
      "Checking column 240 for inf values\n",
      "Checking column 241 for inf values\n",
      "Checking column 242 for inf values\n",
      "Checking column 243 for inf values\n",
      "Checking column 244 for inf values\n",
      "Checking column 245 for inf values\n",
      "Checking column 246 for inf values\n",
      "Checking column 247 for inf values\n",
      "Checking column 248 for inf values\n",
      "Checking column 249 for inf values\n",
      "Checking column 250 for inf values\n",
      "Checking column 251 for inf values\n",
      "Checking column 252 for inf values\n",
      "Checking column 253 for inf values\n",
      "Checking column 254 for inf values\n",
      "Checking column 255 for inf values\n",
      "Checking column 256 for inf values\n",
      "Checking column 257 for inf values\n",
      "Checking column 258 for inf values\n",
      "Checking column 259 for inf values\n",
      "Checking column 260 for inf values\n",
      "Checking column 261 for inf values\n",
      "Checking column 262 for inf values\n",
      "Checking column 263 for inf values\n",
      "Checking column 264 for inf values\n",
      "Checking column 265 for inf values\n",
      "Checking column 266 for inf values\n",
      "Checking column 267 for inf values\n",
      "Checking column 268 for inf values\n",
      "Checking column 269 for inf values\n",
      "Checking column 270 for inf values\n",
      "Checking column 271 for inf values\n",
      "Checking column 272 for inf values\n",
      "Checking column 273 for inf values\n",
      "Checking column 274 for inf values\n",
      "Checking column 275 for inf values\n",
      "Checking column 276 for inf values\n",
      "Checking column 277 for inf values\n",
      "Checking column 278 for inf values\n",
      "Checking column 279 for inf values\n",
      "Checking column 280 for inf values\n",
      "Checking column 281 for inf values\n",
      "Checking column 282 for inf values\n",
      "Checking column 283 for inf values\n",
      "Checking column 284 for inf values\n",
      "Checking column 285 for inf values\n",
      "Checking column 286 for inf values\n",
      "Checking column 287 for inf values\n",
      "Checking column 288 for inf values\n",
      "Checking column 289 for inf values\n",
      "Checking column 290 for inf values\n",
      "Checking column 291 for inf values\n",
      "Checking column 292 for inf values\n",
      "Checking column 293 for inf values\n",
      "Checking column 294 for inf values\n",
      "Checking column 295 for inf values\n",
      "Checking column 296 for inf values\n",
      "Checking column 297 for inf values\n",
      "Checking column 298 for inf values\n",
      "Checking column 299 for inf values\n",
      "Checking column 300 for inf values\n",
      "Checking column 301 for inf values\n",
      "Checking column 302 for inf values\n",
      "Checking column 303 for inf values\n",
      "Checking column 304 for inf values\n",
      "Checking column 305 for inf values\n",
      "Checking column 306 for inf values\n",
      "Checking column 307 for inf values\n",
      "Checking column 308 for inf values\n",
      "Checking column 309 for inf values\n",
      "Checking column 310 for inf values\n",
      "Checking column 311 for inf values\n",
      "Checking column 312 for inf values\n",
      "Checking column 313 for inf values\n",
      "Checking column 314 for inf values\n",
      "Starting imputation with strategy: max_plus, multiplier: 100.0\n",
      "Original data shape: (185003, 315)\n",
      "Original data type: <class 'numpy.ndarray'>\n",
      "Original data dtype: object\n",
      "\n",
      "Data type analysis:\n",
      "  object: 315 columns\n",
      "\n",
      "Total missing values: 3341247\n",
      "Missing values per column (showing first 10 columns with missing data):\n",
      "  Column 47: 4916 (2.7%)\n",
      "  Column 48: 4916 (2.7%)\n",
      "  Column 60: 35078 (19.0%)\n",
      "  Column 61: 19141 (10.3%)\n",
      "  Column 62: 11836 (6.4%)\n",
      "  Column 63: 104517 (56.5%)\n",
      "  Column 64: 92121 (49.8%)\n",
      "  Column 65: 94438 (51.0%)\n",
      "  Column 66: 4 (0.0%)\n",
      "  Column 67: 4 (0.0%)\n",
      "  ... and 198 more columns with missing data\n",
      "Column 47: Imputed 4916 missing values with 49977.010\n",
      "Column 48: Imputed 4916 missing values with 100000.000\n",
      "Column 60: Imputed 35078 missing values with 55720.182\n",
      "Column 61: Imputed 19141 missing values with 1238938.587\n",
      "Column 62: Imputed 11836 missing values with 4428.571\n",
      "Column 63: Imputed 104517 missing values with 53.956\n",
      "Column 64: Imputed 92121 missing values with 459194.145\n",
      "Column 65: Imputed 94438 missing values with 290723.270\n",
      "Column 66: Imputed 4 missing values with 3400.000\n",
      "Column 67: Imputed 4 missing values with 1700.000\n",
      "Column 68: Imputed 4 missing values with 2200.000\n",
      "Column 69: Imputed 4 missing values with 40800.000\n",
      "Column 70: Imputed 4 missing values with 13800.000\n",
      "Column 71: Imputed 4 missing values with 298600.000\n",
      "Column 72: Imputed 4 missing values with 7900.000\n",
      "Column 73: Imputed 4 missing values with 2200.000\n",
      "Column 74: Imputed 4 missing values with 4200.000\n",
      "Column 75: Imputed 4 missing values with 51500.000\n",
      "Column 76: Imputed 4 missing values with 19400.000\n",
      "Column 77: Imputed 4 missing values with 298600.000\n",
      "Column 78: Imputed 4 missing values with 12400.000\n",
      "Column 79: Imputed 4 missing values with 6100.000\n",
      "Column 80: Imputed 4 missing values with 10500.000\n",
      "Column 81: Imputed 4 missing values with 63200.000\n",
      "Column 82: Imputed 4 missing values with 52700.000\n",
      "Column 83: Imputed 4 missing values with 321200.000\n",
      "Column 84: Imputed 4 missing values with 9842.578\n",
      "Column 85: Imputed 4 missing values with 5746.340\n",
      "Column 86: Imputed 4 missing values with 714900.000\n",
      "Column 87: Imputed 5326 missing values with 100000.000\n",
      "Column 88: Imputed 4 missing values with 0.038\n",
      "Column 89: Imputed 4 missing values with 31002.626\n",
      "Column 90: Imputed 4 missing values with 6553500.000\n",
      "Column 91: Imputed 36526 missing values with 55720.182\n",
      "Column 92: Imputed 21224 missing values with 1238938.587\n",
      "Column 93: Imputed 13562 missing values with 4428.571\n",
      "Column 94: Imputed 104744 missing values with 53.956\n",
      "Column 95: Imputed 92402 missing values with 459194.145\n",
      "Column 96: Imputed 94309 missing values with 290723.270\n",
      "Column 118: Imputed 3802 missing values with 100000.000\n",
      "Column 122: Imputed 23756 missing values with 50688.568\n",
      "Column 123: Imputed 7977 missing values with 1238938.587\n",
      "Column 125: Imputed 93947 missing values with 53.895\n",
      "Column 126: Imputed 83097 missing values with 452194.325\n",
      "Column 127: Imputed 84504 missing values with 290723.270\n",
      "Column 128: Imputed 4 missing values with 2800.000\n",
      "Column 129: Imputed 4 missing values with 800.000\n",
      "Column 130: Imputed 4 missing values with 2158.333\n",
      "Column 131: Imputed 4 missing values with 16400.000\n",
      "Column 132: Imputed 4 missing values with 9200.000\n",
      "Column 133: Imputed 4 missing values with 34050.000\n",
      "Column 134: Imputed 4 missing values with 6800.000\n",
      "Column 135: Imputed 4 missing values with 1175.000\n",
      "Column 136: Imputed 4 missing values with 4458.333\n",
      "Column 137: Imputed 4 missing values with 16400.000\n",
      "Column 138: Imputed 4 missing values with 13316.667\n",
      "Column 139: Imputed 4 missing values with 34050.000\n",
      "Column 140: Imputed 4 missing values with 9541.667\n",
      "Column 141: Imputed 4 missing values with 5500.000\n",
      "Column 142: Imputed 4 missing values with 5833.333\n",
      "Column 143: Imputed 4 missing values with 45983.333\n",
      "Column 144: Imputed 4 missing values with 52700.000\n",
      "Column 145: Imputed 4 missing values with 36550.000\n",
      "Column 146: Imputed 4 missing values with 7249.119\n",
      "Column 147: Imputed 4 missing values with 4001.317\n",
      "Column 148: Imputed 4 missing values with 622694.771\n",
      "Column 149: Imputed 4019 missing values with 100000.000\n",
      "Column 150: Imputed 4 missing values with 0.015\n",
      "Column 151: Imputed 4 missing values with 30701.252\n",
      "Column 152: Imputed 4 missing values with 6553500.000\n",
      "Column 153: Imputed 24587 missing values with 50688.568\n",
      "Column 154: Imputed 7981 missing values with 1238938.587\n",
      "Column 155: Imputed 4 missing values with 4357.143\n",
      "Column 156: Imputed 95091 missing values with 53.834\n",
      "Column 157: Imputed 83219 missing values with 452194.325\n",
      "Column 158: Imputed 84625 missing values with 290723.270\n",
      "Column 159: Imputed 30076 missing values with 100.000\n",
      "Column 161: Imputed 15724 missing values with 100000.000\n",
      "Column 162: Imputed 15724 missing values with 83580.068\n",
      "Column 163: Imputed 5974 missing values with 1101.027\n",
      "Column 164: Imputed 6055 missing values with 1071.781\n",
      "Column 166: Imputed 3121 missing values with 3900.000\n",
      "Column 167: Imputed 6602 missing values with 4500.000\n",
      "Column 168: Imputed 6602 missing values with 3400.000\n",
      "Column 169: Imputed 3121 missing values with 1700.000\n",
      "Column 170: Imputed 6602 missing values with 1400.000\n",
      "Column 171: Imputed 6602 missing values with 1900.000\n",
      "Column 172: Imputed 3121 missing values with 2800.000\n",
      "Column 173: Imputed 6602 missing values with 2200.000\n",
      "Column 174: Imputed 6602 missing values with 2200.000\n",
      "Column 175: Imputed 3121 missing values with 40800.000\n",
      "Column 176: Imputed 6602 missing values with 46600.000\n",
      "Column 177: Imputed 6602 missing values with 32000.000\n",
      "Column 178: Imputed 3121 missing values with 15000.000\n",
      "Column 179: Imputed 6602 missing values with 19600.000\n",
      "Column 180: Imputed 6602 missing values with 26000.000\n",
      "Column 181: Imputed 3121 missing values with 298600.000\n",
      "Column 182: Imputed 6602 missing values with 104300.000\n",
      "Column 183: Imputed 6602 missing values with 88200.000\n",
      "Column 184: Imputed 3121 missing values with 6800.000\n",
      "Column 185: Imputed 6602 missing values with 7900.000\n",
      "Column 186: Imputed 6602 missing values with 6800.000\n",
      "Column 187: Imputed 3121 missing values with 2200.000\n",
      "Column 188: Imputed 6602 missing values with 2700.000\n",
      "Column 189: Imputed 6602 missing values with 2300.000\n",
      "Column 190: Imputed 3121 missing values with 7000.000\n",
      "Column 191: Imputed 6602 missing values with 4200.000\n",
      "Column 192: Imputed 6602 missing values with 5300.000\n",
      "Column 193: Imputed 3121 missing values with 61100.000\n",
      "Column 194: Imputed 6602 missing values with 48100.000\n",
      "Column 195: Imputed 6602 missing values with 32000.000\n",
      "Column 196: Imputed 3121 missing values with 16100.000\n",
      "Column 197: Imputed 6602 missing values with 44300.000\n",
      "Column 198: Imputed 6602 missing values with 35600.000\n",
      "Column 199: Imputed 3121 missing values with 298600.000\n",
      "Column 200: Imputed 6602 missing values with 159500.000\n",
      "Column 201: Imputed 6602 missing values with 88200.000\n",
      "Column 202: Imputed 3121 missing values with 15200.000\n",
      "Column 203: Imputed 6602 missing values with 12400.000\n",
      "Column 204: Imputed 6602 missing values with 11000.000\n",
      "Column 205: Imputed 3121 missing values with 6100.000\n",
      "Column 206: Imputed 6602 missing values with 9600.000\n",
      "Column 207: Imputed 6602 missing values with 6100.000\n",
      "Column 208: Imputed 3121 missing values with 8100.000\n",
      "Column 209: Imputed 6602 missing values with 8300.000\n",
      "Column 210: Imputed 6602 missing values with 10900.000\n",
      "Column 211: Imputed 3121 missing values with 84100.000\n",
      "Column 212: Imputed 6602 missing values with 63200.000\n",
      "Column 213: Imputed 6602 missing values with 45100.000\n",
      "Column 214: Imputed 3121 missing values with 52700.000\n",
      "Column 215: Imputed 6602 missing values with 52700.000\n",
      "Column 216: Imputed 6602 missing values with 52700.000\n",
      "Column 217: Imputed 3121 missing values with 321200.000\n",
      "Column 218: Imputed 6602 missing values with 159500.000\n",
      "Column 219: Imputed 6602 missing values with 88200.000\n",
      "Column 220: Imputed 3121 missing values with 5263.820\n",
      "Column 221: Imputed 6602 missing values with 4945.219\n",
      "Column 222: Imputed 6602 missing values with 4705.422\n",
      "Column 223: Imputed 3121 missing values with 3524.801\n",
      "Column 224: Imputed 6602 missing values with 3799.994\n",
      "Column 225: Imputed 6602 missing values with 6308.622\n",
      "Column 226: Imputed 3121 missing values with 714900.000\n",
      "Column 227: Imputed 6602 missing values with 723400.000\n",
      "Column 228: Imputed 6602 missing values with 731660.000\n",
      "Column 229: Imputed 7898 missing values with 100000.000\n",
      "Column 230: Imputed 11258 missing values with 100000.000\n",
      "Column 231: Imputed 11512 missing values with 100000.000\n",
      "Column 232: Imputed 3121 missing values with 0.042\n",
      "Column 233: Imputed 6602 missing values with 0.035\n",
      "Column 234: Imputed 6602 missing values with 0.035\n",
      "Column 235: Imputed 3121 missing values with 31023.234\n",
      "Column 236: Imputed 6602 missing values with 31107.353\n",
      "Column 237: Imputed 6602 missing values with 30939.438\n",
      "Column 238: Imputed 3121 missing values with 6553500.000\n",
      "Column 239: Imputed 6602 missing values with 6553500.000\n",
      "Column 240: Imputed 6602 missing values with 6553500.000\n",
      "Column 241: Imputed 34277 missing values with 55720.182\n",
      "Column 242: Imputed 37415 missing values with 55720.182\n",
      "Column 243: Imputed 37415 missing values with 55720.182\n",
      "Column 244: Imputed 18343 missing values with 1238938.587\n",
      "Column 245: Imputed 21745 missing values with 1238938.587\n",
      "Column 246: Imputed 21745 missing values with 1238938.587\n",
      "Column 247: Imputed 10892 missing values with 4428.571\n",
      "Column 248: Imputed 14373 missing values with 4428.571\n",
      "Column 249: Imputed 14373 missing values with 4428.571\n",
      "Column 250: Imputed 103764 missing values with 53.956\n",
      "Column 251: Imputed 105528 missing values with 53.956\n",
      "Column 252: Imputed 105528 missing values with 53.956\n",
      "Column 253: Imputed 92634 missing values with 459194.145\n",
      "Column 254: Imputed 95883 missing values with 459194.145\n",
      "Column 255: Imputed 96982 missing values with 459194.145\n",
      "Column 256: Imputed 94872 missing values with 290723.270\n",
      "Column 257: Imputed 99131 missing values with 290723.270\n",
      "Column 258: Imputed 98354 missing values with 290723.270\n",
      "Column 259: Imputed 3121 missing values with 2800.000\n",
      "Column 260: Imputed 3121 missing values with 800.000\n",
      "Column 261: Imputed 3121 missing values with 2158.333\n",
      "Column 262: Imputed 3121 missing values with 16400.000\n",
      "Column 263: Imputed 3121 missing values with 9200.000\n",
      "Column 264: Imputed 3121 missing values with 34050.000\n",
      "Column 265: Imputed 3121 missing values with 6800.000\n",
      "Column 266: Imputed 3121 missing values with 1175.000\n",
      "Column 267: Imputed 3121 missing values with 4458.333\n",
      "Column 268: Imputed 3121 missing values with 16400.000\n",
      "Column 269: Imputed 3121 missing values with 13958.333\n",
      "Column 270: Imputed 3121 missing values with 34050.000\n",
      "Column 271: Imputed 3121 missing values with 9541.667\n",
      "Column 272: Imputed 3121 missing values with 5500.000\n",
      "Column 273: Imputed 3121 missing values with 5933.333\n",
      "Column 274: Imputed 3121 missing values with 45983.333\n",
      "Column 275: Imputed 3121 missing values with 52700.000\n",
      "Column 276: Imputed 3121 missing values with 36550.000\n",
      "Column 277: Imputed 3121 missing values with 7249.119\n",
      "Column 278: Imputed 3121 missing values with 2591.208\n",
      "Column 279: Imputed 3121 missing values with 618974.428\n",
      "Column 280: Imputed 6883 missing values with 100000.000\n",
      "Column 281: Imputed 3121 missing values with 0.015\n",
      "Column 282: Imputed 3121 missing values with 30701.252\n",
      "Column 283: Imputed 3121 missing values with 6553500.000\n",
      "Column 284: Imputed 24948 missing values with 53204.375\n",
      "Column 285: Imputed 11020 missing values with 1238938.587\n",
      "Column 286: Imputed 3121 missing values with 4392.857\n",
      "Column 287: Imputed 93197 missing values with 53.895\n",
      "Column 288: Imputed 85163 missing values with 420930.390\n",
      "Column 289: Imputed 86393 missing values with 290723.270\n",
      "Column 291: Imputed 4081 missing values with 100.000\n",
      "Column 292: Imputed 8162 missing values with 100.000\n",
      "Column 293: Imputed 12243 missing values with 100.000\n",
      "\n",
      "Imputation Summary:\n",
      "  Imputed 208 columns total (showing first 5):\n",
      "    Column 47: [0.255, 499.770] → imputed with 49977.010\n",
      "    Column 48: [20.771, 1000.000] → imputed with 100000.000\n",
      "    Column 60: [-6.687, 557.202] → imputed with 55720.182\n",
      "    Column 61: [828.861, 12389.386] → imputed with 1238938.587\n",
      "    Column 62: [0.000, 44.286] → imputed with 4428.571\n",
      "    ... and 203 more columns\n",
      "\n",
      "Remaining missing values after imputation: 0\n",
      "Output data shape: (185003, 315)\n",
      "Output data type: float64\n"
     ]
    }
   ],
   "source": [
    "# l1 index is all columns except l2 index\n",
    "l1_index = [i for i in range(df.shape[1]) if i not in l2_index]\n",
    "\n",
    "X = df.values\n",
    "X = comp_impute(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ab0f137",
   "metadata": {},
   "outputs": [],
   "source": [
    "nowcasting = False\n",
    "max_depth = None\n",
    "results_df= pd.DataFrame(columns=['year','precision(0)', 'recall(0)', 'f1(0)', 'precision(1)', 'recall(1)', 'f1(1)', 'precision_base(0)', 'recall_base(0)', 'f1_base(0)', 'precision_base(1)', 'recall_base(1)', 'f1_base(1)',\n",
    "                                  'num_samples(0)','num_samples(1)'])\n",
    "\n",
    "y_pred_test = pd.DataFrame(columns=['year', 'month','adm_code','fews_ipc_crisis_pred','fews_ipc_crisis_true'])\n",
    "#train test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "448f93b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_dir: result_GeoRF_43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\swl00\\IFPRI Dropbox\\Weilun Shi\\Google fund\\Analysis\\2.source_code\\Step5_Geo_RF_trial\\Food_Crisis_Cluster\\helper.py:235: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  s_branch[''][:gid_list.shape[0]] = gid_list\n",
      "c:\\Users\\swl00\\IFPRI Dropbox\\Weilun Shi\\Google fund\\Analysis\\2.source_code\\Step5_Geo_RF_trial\\Food_Crisis_Cluster\\partition_opt.py:465: RuntimeWarning: invalid value encountered in divide\n",
      "  q_init = np.nan_to_num(c/b)\n",
      "c:\\Users\\swl00\\IFPRI Dropbox\\Weilun Shi\\Google fund\\Analysis\\2.source_code\\Step5_Geo_RF_trial\\Food_Crisis_Cluster\\partition_opt.py:465: RuntimeWarning: invalid value encountered in divide\n",
      "  q_init = np.nan_to_num(c/b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_dir: result_GeoRF_43\n",
      "Printing to file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swl00\\AppData\\Local\\Temp\\ipykernel_22172\\939127335.py:132: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, pd.DataFrame({\n",
      "C:\\Users\\swl00\\AppData\\Local\\Temp\\ipykernel_22172\\939127335.py:150: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y_pred_test = pd.concat([y_pred_test, pd.DataFrame({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_dir: result_GeoRF_44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\swl00\\IFPRI Dropbox\\Weilun Shi\\Google fund\\Analysis\\2.source_code\\Step5_Geo_RF_trial\\Food_Crisis_Cluster\\helper.py:235: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  s_branch[''][:gid_list.shape[0]] = gid_list\n",
      "c:\\Users\\swl00\\IFPRI Dropbox\\Weilun Shi\\Google fund\\Analysis\\2.source_code\\Step5_Geo_RF_trial\\Food_Crisis_Cluster\\partition_opt.py:465: RuntimeWarning: invalid value encountered in divide\n",
      "  q_init = np.nan_to_num(c/b)\n",
      "c:\\Users\\swl00\\IFPRI Dropbox\\Weilun Shi\\Google fund\\Analysis\\2.source_code\\Step5_Geo_RF_trial\\Food_Crisis_Cluster\\partition_opt.py:465: RuntimeWarning: invalid value encountered in divide\n",
      "  q_init = np.nan_to_num(c/b)\n",
      "c:\\Users\\swl00\\IFPRI Dropbox\\Weilun Shi\\Google fund\\Analysis\\2.source_code\\Step5_Geo_RF_trial\\Food_Crisis_Cluster\\partition_opt.py:465: RuntimeWarning: invalid value encountered in divide\n",
      "  q_init = np.nan_to_num(c/b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_dir: result_GeoRF_44\n",
      "Printing to file.\n",
      "model_dir: result_GeoRF_45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\swl00\\IFPRI Dropbox\\Weilun Shi\\Google fund\\Analysis\\2.source_code\\Step5_Geo_RF_trial\\Food_Crisis_Cluster\\helper.py:235: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  s_branch[''][:gid_list.shape[0]] = gid_list\n",
      "c:\\Users\\swl00\\IFPRI Dropbox\\Weilun Shi\\Google fund\\Analysis\\2.source_code\\Step5_Geo_RF_trial\\Food_Crisis_Cluster\\partition_opt.py:465: RuntimeWarning: invalid value encountered in divide\n",
      "  q_init = np.nan_to_num(c/b)\n",
      "c:\\Users\\swl00\\IFPRI Dropbox\\Weilun Shi\\Google fund\\Analysis\\2.source_code\\Step5_Geo_RF_trial\\Food_Crisis_Cluster\\partition_opt.py:465: RuntimeWarning: invalid value encountered in divide\n",
      "  q_init = np.nan_to_num(c/b)\n",
      "c:\\Users\\swl00\\IFPRI Dropbox\\Weilun Shi\\Google fund\\Analysis\\2.source_code\\Step5_Geo_RF_trial\\Food_Crisis_Cluster\\sig_test.py:107: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  es = mean_diff / mean_diff_base_before\n",
      "c:\\Users\\swl00\\IFPRI Dropbox\\Weilun Shi\\Google fund\\Analysis\\2.source_code\\Step5_Geo_RF_trial\\Food_Crisis_Cluster\\partition_opt.py:465: RuntimeWarning: invalid value encountered in divide\n",
      "  q_init = np.nan_to_num(c/b)\n",
      "c:\\Users\\swl00\\IFPRI Dropbox\\Weilun Shi\\Google fund\\Analysis\\2.source_code\\Step5_Geo_RF_trial\\Food_Crisis_Cluster\\sig_test.py:107: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  es = mean_diff / mean_diff_base_before\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_dir: result_GeoRF_45\n",
      "Printing to file.\n",
      "model_dir: result_GeoRF_46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\swl00\\IFPRI Dropbox\\Weilun Shi\\Google fund\\Analysis\\2.source_code\\Step5_Geo_RF_trial\\Food_Crisis_Cluster\\helper.py:235: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  s_branch[''][:gid_list.shape[0]] = gid_list\n",
      "c:\\Users\\swl00\\IFPRI Dropbox\\Weilun Shi\\Google fund\\Analysis\\2.source_code\\Step5_Geo_RF_trial\\Food_Crisis_Cluster\\partition_opt.py:465: RuntimeWarning: invalid value encountered in divide\n",
      "  q_init = np.nan_to_num(c/b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_dir: result_GeoRF_46\n",
      "Printing to file.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO: two layer model not yet completed\n",
    "# TODO: why some f1, recall, and sensitivity is higher in the base model?\n",
    "# Updated main notebook code with consistent 2-layer evaluation\n",
    "\n",
    "if nowcasting:\n",
    "    for year in range(2021,2025):\n",
    "        # Train-test split\n",
    "        (Xtrain, ytrain, Xtrain_loc, Xtrain_group,\n",
    "        Xtest, ytest, Xtest_loc, Xtest_group) = train_test_split_by_year(\n",
    "            X, y, X_loc, X_group, df['years'].values, test_year=year)\n",
    "        ytrain = ytrain.astype(int)\n",
    "        ytest  = ytest.astype(int)\n",
    "        \n",
    "        # Split features for both layers\n",
    "        Xtrain_L1 = Xtrain[:, l1_index]\n",
    "        Xtrain_L2 = Xtrain[:, l2_index]\n",
    "        Xtest_L1 = Xtest[:, l1_index]\n",
    "        Xtest_L2 = Xtest[:, l2_index]\n",
    "        \n",
    "        # Create and train 2-layer GeoRF model\n",
    "        georf_2layer = GeoRF(\n",
    "            min_model_depth=MIN_DEPTH,\n",
    "            max_model_depth=MAX_DEPTH, \n",
    "            n_jobs=N_JOBS,\n",
    "            max_depth=max_depth\n",
    "        )\n",
    "        \n",
    "        # Train 2-layer model\n",
    "        georf_2layer.fit_2layer(Xtrain_L1, Xtrain_L2, ytrain, Xtrain_group, val_ratio=VAL_RATIO)\n",
    "        \n",
    "        # Get predictions\n",
    "        ypred = georf_2layer.predict_2layer(Xtest_L1, Xtest_L2, Xtest_group, correction_strategy='flip')\n",
    "        \n",
    "        # Evaluate: This now compares 2-layer GeoRF vs 2-layer Base RF\n",
    "        (pre, rec, f1, pre_base, rec_base, f1_base) = georf_2layer.evaluate_2layer(\n",
    "            X_L1_test=Xtest_L1,\n",
    "            X_L2_test=Xtest_L2, \n",
    "            y_test=ytest,\n",
    "            X_group_test=Xtest_group,\n",
    "            X_L1_train=Xtrain_L1,\n",
    "            X_L2_train=Xtrain_L2,\n",
    "            y_train=ytrain, \n",
    "            X_group_train=Xtrain_group,\n",
    "            correction_strategy='flip',\n",
    "            print_to_file=True\n",
    "        )\n",
    "        \n",
    "        # Calculate weighted averages (same as your forecasting model)\n",
    "        nsample_class = np.bincount(ytest)\n",
    "        \n",
    "        # Store results\n",
    "        results_df = pd.concat([results_df, pd.DataFrame({\n",
    "            'year': year,\n",
    "            'precision(0)': pre[0],\n",
    "            'precision(1)': pre[1],\n",
    "            'recall(0)': rec[0],\n",
    "            'recall(1)': rec[1],\n",
    "            'f1(0)': f1[0],\n",
    "            'f1(1)': f1[1],\n",
    "            'precision_base(0)': pre_base[0],\n",
    "            'precision_base(1)': pre_base[1],\n",
    "            'recall_base(0)': rec_base[0], \n",
    "            'recall_base(1)': rec_base[1],\n",
    "            'f1_base(0)': f1_base[0],\n",
    "            'f1_base(1)': f1_base[1],\n",
    "            'num_samples(0)': nsample_class[0],\n",
    "            'num_samples(1)': nsample_class[1]\n",
    "        }, index=[0])], ignore_index=True)\n",
    "        \n",
    "        y_pred_test = pd.concat([y_pred_test, pd.DataFrame({\n",
    "            'year': year, \n",
    "            'month': Xtest[:,1], \n",
    "            'adm_code': Xtest[:,0], \n",
    "            'fews_ipc_crisis_pred': ypred, \n",
    "            'fews_ipc_crisis_true': ytest\n",
    "        })], ignore_index=True)\n",
    "        \n",
    "        print(f\"Year {year} - 2-Layer GeoRF F1: {f1:.4f}, 2-Layer Base RF F1: {f1_base:.4f}\")\n",
    "    pred_test_name = 'y_pred_test_g'\n",
    "    results_df_name = 'results_df_g'\n",
    "    if assignment=='polygons':\n",
    "        pred_test_name = pred_test_name + 'pn'\n",
    "        results_df_name = results_df_name + 'pn'\n",
    "    elif assignment == 'grid':\n",
    "        pred_test_name = pred_test_name + 'gn'\n",
    "        results_df_name = results_df_name + 'gn'\n",
    "    elif assignment == 'country':\n",
    "        pred_test_name = pred_test_name + 'cn'\n",
    "        results_df_name = results_df_name + 'cn'\n",
    "    elif assignment == 'AEZ':\n",
    "        pred_test_name = pred_test_name + 'aen'\n",
    "        results_df_name = results_df_name + 'aen'\n",
    "    elif assignment == 'country_AEZ':\n",
    "        pred_test_name = pred_test_name + 'caen'\n",
    "        results_df_name = results_df_name + 'caen'\n",
    "    elif assignment == 'geokmeans':\n",
    "        pred_test_name = pred_test_name + 'gkn'\n",
    "        results_df_name = results_df_name + 'gkn'\n",
    "    elif assignment == 'all_kmeans':\n",
    "        pred_test_name = pred_test_name + 'akn'\n",
    "        results_df_name = results_df_name + 'akn'\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown assignment method: {assignment}\")\n",
    "\n",
    "    if max_depth is not None:\n",
    "        pred_test_name = pred_test_name + f'_d{max_depth}.csv'\n",
    "        results_df_name = results_df_name + f'_d{max_depth}.csv'\n",
    "    else:\n",
    "        pred_test_name = pred_test_name + '.csv'\n",
    "        results_df_name = results_df_name + '.csv'\n",
    "    y_pred_test.to_csv(pred_test_name, index=False)\n",
    "    results_df.to_csv(results_df_name, index=False)\n",
    "\n",
    "else:\n",
    "    # Your existing forecasting code remains exactly the same\n",
    "    for year in range(2021,2025):\n",
    "        (Xtrain, ytrain, Xtrain_loc, Xtrain_group,\n",
    "        Xtest, ytest, Xtest_loc, Xtest_group) = train_test_split_by_year(\n",
    "            X, y, X_loc, X_group, df['years'].values, test_year=year)\n",
    "        ytrain = ytrain.astype(int)\n",
    "        ytest  = ytest.astype(int)\n",
    "        \n",
    "        georf = GeoRF(min_model_depth = MIN_DEPTH, max_model_depth = MAX_DEPTH, n_jobs = N_JOBS, max_depth=max_depth)\n",
    "        georf.fit(Xtrain, ytrain, Xtrain_group, val_ratio = VAL_RATIO)\n",
    "        ypred = georf.predict(Xtest, Xtest_group)\n",
    "        \n",
    "        # This returns single-layer GeoRF vs single-layer Base RF\n",
    "        (pre, rec, f1, pre_base, rec_base, f1_base) = georf.evaluate(Xtest, ytest, Xtest_group, eval_base = True, print_to_file = True)\n",
    "        \n",
    "        nsample_class = np.bincount(ytest)\n",
    "        \n",
    "        results_df = pd.concat([results_df, pd.DataFrame({\n",
    "            'year': year,\n",
    "            'precision(0)': pre[0],\n",
    "            'precision(1)': pre[1],\n",
    "            'recall(0)': rec[0],\n",
    "            'recall(1)': rec[1],\n",
    "            'f1(0)': f1[0],\n",
    "            'f1(1)': f1[1],\n",
    "            'precision_base(0)': pre_base[0],\n",
    "            'precision_base(1)': pre_base[1],\n",
    "            'recall_base(0)': rec_base[0], \n",
    "            'recall_base(1)': rec_base[1],\n",
    "            'f1_base(0)': f1_base[0],\n",
    "            'f1_base(1)': f1_base[1],\n",
    "            'num_samples(0)': nsample_class[0],\n",
    "            'num_samples(1)': nsample_class[1]\n",
    "        }, index=[0])], ignore_index=True)\n",
    "        \n",
    "        y_pred_test = pd.concat([y_pred_test, pd.DataFrame({\n",
    "            'year': year, \n",
    "            'month': Xtest[:,1], \n",
    "            'adm_code': Xtest[:,0], \n",
    "            'fews_ipc_crisis_pred': ypred, \n",
    "            'fews_ipc_crisis_true': ytest\n",
    "        })], ignore_index=True)\n",
    "        \n",
    "    pred_test_name = 'y_pred_test_g'\n",
    "    results_df_name = 'results_df_g'\n",
    "    if assignment=='polygons':\n",
    "        pred_test_name = pred_test_name + 'pn'\n",
    "        results_df_name = results_df_name + 'pn'\n",
    "    elif assignment == 'grid':\n",
    "        pred_test_name = pred_test_name + 'gn'\n",
    "        results_df_name = results_df_name + 'gn'\n",
    "    elif assignment == 'country':\n",
    "        pred_test_name = pred_test_name + 'cn'\n",
    "        results_df_name = results_df_name + 'cn'\n",
    "    elif assignment == 'AEZ':\n",
    "        pred_test_name = pred_test_name + 'aen'\n",
    "        results_df_name = results_df_name + 'aen'\n",
    "    elif assignment == 'country_AEZ':\n",
    "        pred_test_name = pred_test_name + 'caen'\n",
    "        results_df_name = results_df_name + 'caen'\n",
    "    elif assignment == 'geokmeans':\n",
    "        pred_test_name = pred_test_name + 'gkn'\n",
    "        results_df_name = results_df_name + 'gkn'\n",
    "    elif assignment == 'all_kmeans':\n",
    "        pred_test_name = pred_test_name + 'akn'\n",
    "        results_df_name = results_df_name + 'akn'\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown assignment method: {assignment}\")\n",
    "\n",
    "    if max_depth is not None:\n",
    "        pred_test_name = pred_test_name + f'_d{max_depth}.csv'\n",
    "        results_df_name = results_df_name + f'_d{max_depth}.csv'\n",
    "    else:\n",
    "        pred_test_name = pred_test_name + '.csv'\n",
    "        results_df_name = results_df_name + '.csv'\n",
    "    y_pred_test.to_csv(pred_test_name, index=False)\n",
    "    results_df.to_csv(results_df_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
